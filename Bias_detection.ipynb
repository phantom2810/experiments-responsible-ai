{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293f91f8-ddab-4e8b-a3bd-61cbf2c37db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ TrustyAI Bias Detection Guide\n",
      "TrustyAI version: 0.6.1\n",
      "==================================================\n",
      "\n",
      "üìä CREATING BIASED DATASET:\n",
      "------------------------------\n",
      "‚úÖ Created biased dataset: (2000, 6)\n",
      "Loan approval rate: 87.3%\n",
      "\n",
      "üö® RAW DATA BIAS ANALYSIS:\n",
      "Loan approval rates by demographic:\n",
      "\n",
      "By Gender:\n",
      "        Count  Approval_Rate\n",
      "gender                      \n",
      "Female    817       0.757650\n",
      "Male     1183       0.952663\n",
      "\n",
      "By Race:\n",
      "          Count  Approval_Rate\n",
      "race                          \n",
      "Asian       403       0.903226\n",
      "Black       398       0.763819\n",
      "Hispanic    189       0.777778\n",
      "White      1010       0.921782\n",
      "\n",
      "‚öôÔ∏è PREPARING DATA FOR ML MODEL:\n",
      "-----------------------------------\n",
      "‚úÖ Model trained - Accuracy: 0.883\n",
      "\n",
      "üìà STATISTICAL BIAS DETECTION:\n",
      "========================================\n",
      "üîç DEMOGRAPHIC PARITY ANALYSIS:\n",
      "(Equal approval rates across groups)\n",
      "\n",
      "Gender Bias Metrics:\n",
      "  Female: 78.9% approval rate (251 samples)\n",
      "  Male: 96.6% approval rate (349 samples)\n",
      "\n",
      "Race Bias Metrics:\n",
      "  Asian: 93.2% approval rate (133 samples)\n",
      "  White: 92.7% approval rate (286 samples)\n",
      "  Black: 79.4% approval rate (107 samples)\n",
      "  Hispanic: 82.4% approval rate (74 samples)\n",
      "\n",
      "‚öñÔ∏è DISPARATE IMPACT RATIOS:\n",
      "Gender Disparate Impact: 0.817 (1.0 = perfect fairness)\n",
      "Race Disparate Impact: 0.852 (1.0 = perfect fairness)\n",
      "\n",
      "üö® BIAS ALERTS:\n",
      "‚ö†Ô∏è  MODERATE GENDER BIAS DETECTED (DI: 0.817)\n",
      "‚ö†Ô∏è  MODERATE RACIAL BIAS DETECTED (DI: 0.852)\n",
      "\n",
      "üéØ TRUSTYAI EXPLANATION-BASED BIAS DETECTION:\n",
      "=======================================================\n",
      "‚úÖ Created 20 TrustyAI prediction inputs\n",
      "üîç ANALYZING EXPLANATIONS BY GENDER:\n",
      "Average feature importance by gender:\n",
      "  Female:\n",
      "    gender_encoded: 2.595\n",
      "    race_encoded: 2.582\n",
      "    education: 2.576\n",
      "    experience: -0.007\n",
      "    age: -0.003\n",
      "  Male:\n",
      "    education: 2.426\n",
      "    gender_encoded: 2.417\n",
      "    race_encoded: 2.410\n",
      "    experience: 0.068\n",
      "    age: 0.065\n",
      "\n",
      "üîç BIAS DETECTION IN FEATURE IMPORTANCE:\n",
      "=============================================\n",
      "üö® PROTECTED ATTRIBUTE BIAS ALERTS:\n",
      "  HIGH BIAS: gender_encoded has importance 2.595 for Female\n",
      "  HIGH BIAS: race_encoded has importance 2.582 for Female\n",
      "  HIGH BIAS: gender_encoded has importance 2.417 for Male\n",
      "  HIGH BIAS: race_encoded has importance 2.410 for Male\n",
      "\n",
      "üìã COMPREHENSIVE BIAS DETECTION REPORT:\n",
      "==================================================\n",
      "üìä BIAS DETECTION SUMMARY:\n",
      "   Model Accuracy: 0.883\n",
      "   Overall Bias Severity: MEDIUM\n",
      "   Gender Disparate Impact: 0.817\n",
      "   Race Disparate Impact: 0.852\n",
      "\n",
      "üõ†Ô∏è RECOMMENDATIONS:\n",
      "   1. Consider bias mitigation techniques\n",
      "   2. Monitor model performance across demographics\n",
      "   3. Review feature engineering process\n",
      "   4. Protected attributes showing high importance in explanations\n",
      "\n",
      "üîß BIAS MONITORING HELPER FUNCTIONS:\n",
      "========================================\n",
      "‚úÖ Bias monitoring functions defined:\n",
      "   ‚Ä¢ continuous_bias_monitor()\n",
      "   ‚Ä¢ bias_alert_system()\n",
      "\n",
      "üîç MONITORING SYSTEM TEST:\n",
      "   Detected bias in 0 attributes\n",
      "   Generated 0 alerts\n",
      "\n",
      "üöÄ PRODUCTION DEPLOYMENT RECOMMENDATIONS:\n",
      "==================================================\n",
      "üìã BIAS DETECTION WORKFLOW:\n",
      "1. üìä Pre-deployment:\n",
      "   ‚Ä¢ Run comprehensive bias analysis on test data\n",
      "   ‚Ä¢ Generate bias detection report\n",
      "   ‚Ä¢ Review with legal/compliance team\n",
      "   ‚Ä¢ Set up bias monitoring thresholds\n",
      "\n",
      "2. üîÑ Production monitoring:\n",
      "   ‚Ä¢ Implement continuous bias monitoring\n",
      "   ‚Ä¢ Set up automated alerts for bias drift\n",
      "   ‚Ä¢ Regular bias audits (monthly/quarterly)\n",
      "   ‚Ä¢ Track bias metrics over time\n",
      "\n",
      "3. üõ†Ô∏è Bias mitigation:\n",
      "   ‚Ä¢ Remove protected attributes from features\n",
      "   ‚Ä¢ Use fairness-aware ML algorithms\n",
      "   ‚Ä¢ Implement post-processing fairness corrections\n",
      "   ‚Ä¢ Regular model retraining with bias constraints\n",
      "\n",
      "4. üìà Governance:\n",
      "   ‚Ä¢ Document all bias detection procedures\n",
      "   ‚Ä¢ Maintain audit trails of bias metrics\n",
      "   ‚Ä¢ Regular stakeholder reporting\n",
      "   ‚Ä¢ Legal compliance verification\n",
      "\n",
      "‚úÖ BIAS DETECTION IMPLEMENTATION COMPLETE!\n",
      "==================================================\n",
      "You now have a comprehensive bias detection system using TrustyAI that includes:\n",
      "‚Ä¢ Statistical bias metrics (disparate impact)\n",
      "‚Ä¢ Explanation-based bias detection\n",
      "‚Ä¢ Continuous monitoring capabilities\n",
      "‚Ä¢ Automated alert systems\n",
      "‚Ä¢ Production-ready helper functions\n",
      "‚Ä¢ Comprehensive reporting\n"
     ]
    }
   ],
   "source": [
    "# TrustyAI Bias Detection Implementation Guide\n",
    "# ==========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import TrustyAI components\n",
    "import trustyai\n",
    "from trustyai.explainers import LimeExplainer\n",
    "from trustyai.model import FeatureFactory, PredictionInput\n",
    "from trustyai.utils import TestModels\n",
    "from trustyai.metrics import ExplainabilityMetrics\n",
    "from java.util import Arrays\n",
    "\n",
    "print(f\"üéØ TrustyAI Bias Detection Guide\")\n",
    "print(f\"TrustyAI version: {trustyai.__version__}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Create Biased Dataset for Demonstration\n",
    "print(\"\\nüìä CREATING BIASED DATASET:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Create a dataset with intentional bias patterns\n",
    "np.random.seed(42)\n",
    "n_samples = 2000\n",
    "\n",
    "# Create demographic features (protected attributes)\n",
    "gender = np.random.choice(['Male', 'Female'], n_samples, p=[0.6, 0.4])\n",
    "race = np.random.choice(['White', 'Black', 'Asian', 'Hispanic'], n_samples, p=[0.5, 0.2, 0.2, 0.1])\n",
    "age = np.random.normal(40, 12, n_samples).clip(18, 80)\n",
    "\n",
    "# Create other features\n",
    "education = np.random.choice([0, 1, 2, 3, 4], n_samples, p=[0.1, 0.2, 0.3, 0.3, 0.1])\n",
    "experience = age - 22 + np.random.normal(0, 3, n_samples)\n",
    "experience = experience.clip(0, None)\n",
    "\n",
    "# Create biased target (loan approval) - intentionally discriminatory\n",
    "# Higher approval rates for certain demographics (THIS IS WRONG AND ILLEGAL)\n",
    "loan_approved = np.zeros(n_samples)\n",
    "\n",
    "for i in range(n_samples):\n",
    "    base_score = 0.3 + 0.1 * education[i] + 0.002 * experience[i]\n",
    "    \n",
    "    # Add bias (DO NOT DO THIS IN REAL SYSTEMS!)\n",
    "    if gender[i] == 'Male':\n",
    "        base_score += 0.15  # Male bias\n",
    "    if race[i] == 'White':\n",
    "        base_score += 0.12  # Racial bias\n",
    "    elif race[i] == 'Asian':\n",
    "        base_score += 0.08\n",
    "    \n",
    "    # Add some randomness\n",
    "    base_score += np.random.normal(0, 0.1)\n",
    "    \n",
    "    loan_approved[i] = 1 if base_score > 0.5 else 0\n",
    "\n",
    "# Create DataFrame\n",
    "bias_df = pd.DataFrame({\n",
    "    'gender': gender,\n",
    "    'race': race,\n",
    "    'age': age,\n",
    "    'education': education,\n",
    "    'experience': experience,\n",
    "    'loan_approved': loan_approved\n",
    "})\n",
    "\n",
    "print(f\"‚úÖ Created biased dataset: {bias_df.shape}\")\n",
    "print(f\"Loan approval rate: {loan_approved.mean():.1%}\")\n",
    "\n",
    "# Show bias in raw data\n",
    "print(f\"\\nüö® RAW DATA BIAS ANALYSIS:\")\n",
    "print(\"Loan approval rates by demographic:\")\n",
    "\n",
    "gender_bias = bias_df.groupby('gender')['loan_approved'].agg(['count', 'mean'])\n",
    "gender_bias.columns = ['Count', 'Approval_Rate']\n",
    "print(f\"\\nBy Gender:\")\n",
    "print(gender_bias)\n",
    "\n",
    "race_bias = bias_df.groupby('race')['loan_approved'].agg(['count', 'mean'])\n",
    "race_bias.columns = ['Count', 'Approval_Rate']\n",
    "print(f\"\\nBy Race:\")\n",
    "print(race_bias)\n",
    "\n",
    "# 2. Prepare Data for ML Model\n",
    "print(f\"\\n‚öôÔ∏è PREPARING DATA FOR ML MODEL:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for col in ['gender', 'race']:\n",
    "    le = LabelEncoder()\n",
    "    bias_df[f'{col}_encoded'] = le.fit_transform(bias_df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Prepare features and target\n",
    "feature_columns = ['age', 'education', 'experience', 'gender_encoded', 'race_encoded']\n",
    "X = bias_df[feature_columns].values\n",
    "y = bias_df['loan_approved'].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"‚úÖ Model trained - Accuracy: {model.score(X_test, y_test):.3f}\")\n",
    "\n",
    "# 3. Statistical Bias Detection\n",
    "print(f\"\\nüìà STATISTICAL BIAS DETECTION:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Get predictions for test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Create test dataframe with predictions\n",
    "test_df = pd.DataFrame(X_test, columns=feature_columns)\n",
    "test_df['actual'] = y_test\n",
    "test_df['predicted'] = y_pred\n",
    "test_df['prediction_probability'] = y_pred_proba\n",
    "\n",
    "# Decode categorical variables for analysis\n",
    "test_df['gender'] = label_encoders['gender'].inverse_transform(test_df['gender_encoded'].astype(int))\n",
    "test_df['race'] = label_encoders['race'].inverse_transform(test_df['race_encoded'].astype(int))\n",
    "\n",
    "def calculate_bias_metrics(df, protected_attr, outcome_col='predicted'):\n",
    "    \"\"\"Calculate comprehensive bias metrics\"\"\"\n",
    "    groups = df[protected_attr].unique()\n",
    "    metrics = {}\n",
    "    \n",
    "    for group in groups:\n",
    "        group_data = df[df[protected_attr] == group]\n",
    "        total_count = len(group_data)\n",
    "        positive_predictions = group_data[outcome_col].sum()\n",
    "        approval_rate = positive_predictions / total_count if total_count > 0 else 0\n",
    "        \n",
    "        metrics[group] = {\n",
    "            'count': total_count,\n",
    "            'positive_predictions': positive_predictions,\n",
    "            'approval_rate': approval_rate\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate bias metrics\n",
    "print(\"üîç DEMOGRAPHIC PARITY ANALYSIS:\")\n",
    "print(\"(Equal approval rates across groups)\")\n",
    "\n",
    "gender_metrics = calculate_bias_metrics(test_df, 'gender')\n",
    "race_metrics = calculate_bias_metrics(test_df, 'race')\n",
    "\n",
    "print(f\"\\nGender Bias Metrics:\")\n",
    "for gender, metrics in gender_metrics.items():\n",
    "    print(f\"  {gender}: {metrics['approval_rate']:.1%} approval rate ({metrics['count']} samples)\")\n",
    "\n",
    "print(f\"\\nRace Bias Metrics:\")\n",
    "for race, metrics in race_metrics.items():\n",
    "    print(f\"  {race}: {metrics['approval_rate']:.1%} approval rate ({metrics['count']} samples)\")\n",
    "\n",
    "# Calculate disparate impact ratios\n",
    "def calculate_disparate_impact(metrics):\n",
    "    \"\"\"Calculate disparate impact ratios (should be close to 1.0 for fairness)\"\"\"\n",
    "    rates = [m['approval_rate'] for m in metrics.values()]\n",
    "    max_rate = max(rates)\n",
    "    min_rate = min(rates) \n",
    "    \n",
    "    if max_rate == 0:\n",
    "        return float('inf')\n",
    "    \n",
    "    return min_rate / max_rate\n",
    "\n",
    "gender_di = calculate_disparate_impact(gender_metrics)\n",
    "race_di = calculate_disparate_impact(race_metrics)\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è DISPARATE IMPACT RATIOS:\")\n",
    "print(f\"Gender Disparate Impact: {gender_di:.3f} (1.0 = perfect fairness)\")\n",
    "print(f\"Race Disparate Impact: {race_di:.3f} (1.0 = perfect fairness)\")\n",
    "\n",
    "# Flag concerning bias levels\n",
    "print(f\"\\nüö® BIAS ALERTS:\")\n",
    "if gender_di < 0.8:\n",
    "    print(f\"‚ùå SEVERE GENDER BIAS DETECTED (DI: {gender_di:.3f})\")\n",
    "elif gender_di < 0.9:\n",
    "    print(f\"‚ö†Ô∏è  MODERATE GENDER BIAS DETECTED (DI: {gender_di:.3f})\")\n",
    "else:\n",
    "    print(f\"‚úÖ ACCEPTABLE GENDER FAIRNESS (DI: {gender_di:.3f})\")\n",
    "\n",
    "if race_di < 0.8:\n",
    "    print(f\"‚ùå SEVERE RACIAL BIAS DETECTED (DI: {race_di:.3f})\")\n",
    "elif race_di < 0.9:\n",
    "    print(f\"‚ö†Ô∏è  MODERATE RACIAL BIAS DETECTED (DI: {race_di:.3f})\")\n",
    "else:\n",
    "    print(f\"‚úÖ ACCEPTABLE RACIAL FAIRNESS (DI: {race_di:.3f})\")\n",
    "\n",
    "# 4. TrustyAI Explanation-Based Bias Detection\n",
    "print(f\"\\nüéØ TRUSTYAI EXPLANATION-BASED BIAS DETECTION:\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Convert sample data to TrustyAI format\n",
    "sample_size = 20\n",
    "X_sample = X_test[:sample_size]\n",
    "feature_names = feature_columns\n",
    "\n",
    "prediction_inputs = []\n",
    "for i in range(sample_size):\n",
    "    features = []\n",
    "    for j, feature_name in enumerate(feature_names):\n",
    "        feature = FeatureFactory.newNumericalFeature(feature_name, X_sample[i, j])\n",
    "        features.append(feature)\n",
    "    \n",
    "    pred_input = PredictionInput(features)\n",
    "    prediction_inputs.append(pred_input)\n",
    "\n",
    "# Create TrustyAI model\n",
    "weights = np.array([0.1, 0.3, 0.2, 0.25, 0.15])  # Weights for features\n",
    "trusty_model = TestModels.getLinearModel(weights)\n",
    "\n",
    "print(f\"‚úÖ Created {len(prediction_inputs)} TrustyAI prediction inputs\")\n",
    "\n",
    "# Generate explanations for different demographic groups\n",
    "def analyze_group_explanations(prediction_inputs, test_samples, demographic_col, model):\n",
    "    \"\"\"Analyze explanations across demographic groups\"\"\"\n",
    "    \n",
    "    lime_explainer = LimeExplainer()\n",
    "    group_explanations = {}\n",
    "    \n",
    "    for i in range(min(len(prediction_inputs), len(test_samples))):\n",
    "        # Get demographic group\n",
    "        group = test_samples.iloc[i][demographic_col]\n",
    "        \n",
    "        # Generate explanation\n",
    "        try:\n",
    "            pred_list = Arrays.asList([prediction_inputs[i]])\n",
    "            pred_output = model.predictAsync(pred_list).get().get(0)\n",
    "            lime_result = lime_explainer.explain(prediction_inputs[i], pred_output, model)\n",
    "            \n",
    "            # Extract feature importance\n",
    "            df_result = lime_result.as_dataframe()\n",
    "            if isinstance(df_result, dict):\n",
    "                for key, value in df_result.items():\n",
    "                    if hasattr(value, 'iterrows'):\n",
    "                        if group not in group_explanations:\n",
    "                            group_explanations[group] = []\n",
    "                        \n",
    "                        explanation = {}\n",
    "                        for _, row in value.iterrows():\n",
    "                            explanation[row['Feature']] = row['Saliency']\n",
    "                        group_explanations[group].append(explanation)\n",
    "                        break\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Explanation failed for sample {i}: {e}\")\n",
    "    \n",
    "    return group_explanations\n",
    "\n",
    "# Analyze explanations by gender\n",
    "print(\"üîç ANALYZING EXPLANATIONS BY GENDER:\")\n",
    "gender_explanations = analyze_group_explanations(\n",
    "    prediction_inputs, \n",
    "    test_df.head(sample_size), \n",
    "    'gender', \n",
    "    trusty_model\n",
    ")\n",
    "\n",
    "# Calculate average feature importance by group\n",
    "def calculate_average_importance(group_explanations):\n",
    "    \"\"\"Calculate average feature importance across groups\"\"\"\n",
    "    avg_importance = {}\n",
    "    \n",
    "    for group, explanations in group_explanations.items():\n",
    "        if explanations:\n",
    "            feature_sums = {}\n",
    "            for explanation in explanations:\n",
    "                for feature, importance in explanation.items():\n",
    "                    if feature not in feature_sums:\n",
    "                        feature_sums[feature] = []\n",
    "                    feature_sums[feature].append(importance)\n",
    "            \n",
    "            avg_importance[group] = {}\n",
    "            for feature, values in feature_sums.items():\n",
    "                avg_importance[group][feature] = np.mean(values)\n",
    "    \n",
    "    return avg_importance\n",
    "\n",
    "if gender_explanations:\n",
    "    avg_gender_importance = calculate_average_importance(gender_explanations)\n",
    "    \n",
    "    print(\"Average feature importance by gender:\")\n",
    "    for gender, features in avg_gender_importance.items():\n",
    "        print(f\"  {gender}:\")\n",
    "        for feature, importance in sorted(features.items(), key=lambda x: abs(x[1]), reverse=True):\n",
    "            print(f\"    {feature}: {importance:.3f}\")\n",
    "\n",
    "# 5. Bias Detection in Feature Importance\n",
    "print(f\"\\nüîç BIAS DETECTION IN FEATURE IMPORTANCE:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "def detect_protected_attribute_importance(explanations, protected_features):\n",
    "    \"\"\"Detect if protected attributes have high importance\"\"\"\n",
    "    bias_alerts = []\n",
    "    \n",
    "    for group, features in explanations.items():\n",
    "        for feature in protected_features:\n",
    "            if feature in features:\n",
    "                importance = abs(features[feature])\n",
    "                if importance > 0.5:  # Threshold for concern\n",
    "                    bias_alerts.append({\n",
    "                        'group': group,\n",
    "                        'feature': feature,\n",
    "                        'importance': importance,\n",
    "                        'severity': 'HIGH' if importance > 1.0 else 'MEDIUM'\n",
    "                    })\n",
    "    \n",
    "    return bias_alerts\n",
    "\n",
    "if 'avg_gender_importance' in locals():\n",
    "    protected_features = ['gender_encoded', 'race_encoded']\n",
    "    bias_alerts = detect_protected_attribute_importance(avg_gender_importance, protected_features)\n",
    "    \n",
    "    if bias_alerts:\n",
    "        print(\"üö® PROTECTED ATTRIBUTE BIAS ALERTS:\")\n",
    "        for alert in bias_alerts:\n",
    "            print(f\"  {alert['severity']} BIAS: {alert['feature']} has importance {alert['importance']:.3f} for {alert['group']}\")\n",
    "    else:\n",
    "        print(\"‚úÖ No high importance detected for protected attributes\")\n",
    "\n",
    "# 6. Comprehensive Bias Report\n",
    "print(f\"\\nüìã COMPREHENSIVE BIAS DETECTION REPORT:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def generate_bias_report(statistical_metrics, explanation_metrics, bias_alerts):\n",
    "    \"\"\"Generate comprehensive bias report\"\"\"\n",
    "    \n",
    "    report = {\n",
    "        'timestamp': pd.Timestamp.now(),\n",
    "        'model_accuracy': model.score(X_test, y_test),\n",
    "        'total_samples': len(X_test),\n",
    "        'statistical_bias': {\n",
    "            'gender_disparate_impact': gender_di,\n",
    "            'race_disparate_impact': race_di\n",
    "        },\n",
    "        'bias_severity': 'LOW',\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # Determine overall bias severity\n",
    "    if gender_di < 0.8 or race_di < 0.8:\n",
    "        report['bias_severity'] = 'HIGH'\n",
    "        report['recommendations'].extend([\n",
    "            'URGENT: Review and retrain model without protected attributes',\n",
    "            'Implement fairness constraints in model training',\n",
    "            'Conduct legal review before deployment'\n",
    "        ])\n",
    "    elif gender_di < 0.9 or race_di < 0.9:\n",
    "        report['bias_severity'] = 'MEDIUM'\n",
    "        report['recommendations'].extend([\n",
    "            'Consider bias mitigation techniques',\n",
    "            'Monitor model performance across demographics',\n",
    "            'Review feature engineering process'\n",
    "        ])\n",
    "    else:\n",
    "        report['bias_severity'] = 'LOW'\n",
    "        report['recommendations'].append('Continue monitoring for bias drift')\n",
    "    \n",
    "    # Add explanation-based recommendations\n",
    "    if bias_alerts:\n",
    "        report['recommendations'].append('Protected attributes showing high importance in explanations')\n",
    "    \n",
    "    return report\n",
    "\n",
    "bias_report = generate_bias_report(\n",
    "    {'gender': gender_metrics, 'race': race_metrics},\n",
    "    avg_gender_importance if 'avg_gender_importance' in locals() else {},\n",
    "    bias_alerts if 'bias_alerts' in locals() else []\n",
    ")\n",
    "\n",
    "print(f\"üìä BIAS DETECTION SUMMARY:\")\n",
    "print(f\"   Model Accuracy: {bias_report['model_accuracy']:.3f}\")\n",
    "print(f\"   Overall Bias Severity: {bias_report['bias_severity']}\")\n",
    "print(f\"   Gender Disparate Impact: {bias_report['statistical_bias']['gender_disparate_impact']:.3f}\")\n",
    "print(f\"   Race Disparate Impact: {bias_report['statistical_bias']['race_disparate_impact']:.3f}\")\n",
    "\n",
    "print(f\"\\nüõ†Ô∏è RECOMMENDATIONS:\")\n",
    "for i, rec in enumerate(bias_report['recommendations'], 1):\n",
    "    print(f\"   {i}. {rec}\")\n",
    "\n",
    "# 7. Bias Monitoring Helper Functions\n",
    "print(f\"\\nüîß BIAS MONITORING HELPER FUNCTIONS:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def continuous_bias_monitor(model, X_new, y_new, protected_attributes, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Continuously monitor model for bias on new data\n",
    "    \n",
    "    Args:\n",
    "        model: Trained ML model\n",
    "        X_new: New input data\n",
    "        y_new: New true labels  \n",
    "        protected_attributes: Dict mapping attribute names to column indices\n",
    "        threshold: Disparate impact threshold (default 0.8)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Bias monitoring results\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = model.predict(X_new)\n",
    "    results = {}\n",
    "    \n",
    "    for attr_name, col_idx in protected_attributes.items():\n",
    "        # Get unique groups\n",
    "        groups = np.unique(X_new[:, col_idx])\n",
    "        group_rates = {}\n",
    "        \n",
    "        for group in groups:\n",
    "            mask = X_new[:, col_idx] == group\n",
    "            if np.sum(mask) > 0:  # Ensure group has samples\n",
    "                approval_rate = np.mean(predictions[mask])\n",
    "                group_rates[group] = approval_rate\n",
    "        \n",
    "        # Calculate disparate impact\n",
    "        if len(group_rates) > 1:\n",
    "            rates = list(group_rates.values())\n",
    "            di_ratio = min(rates) / max(rates) if max(rates) > 0 else 0\n",
    "            \n",
    "            results[attr_name] = {\n",
    "                'disparate_impact': di_ratio,\n",
    "                'bias_detected': di_ratio < threshold,\n",
    "                'group_rates': group_rates\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def bias_alert_system(bias_results, alert_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Alert system for bias detection\n",
    "    \n",
    "    Args:\n",
    "        bias_results: Results from continuous_bias_monitor\n",
    "        alert_threshold: Threshold for triggering alerts\n",
    "    \n",
    "    Returns:\n",
    "        list: List of bias alerts\n",
    "    \"\"\"\n",
    "    \n",
    "    alerts = []\n",
    "    \n",
    "    for attribute, results in bias_results.items():\n",
    "        di_ratio = results['disparate_impact']\n",
    "        \n",
    "        if di_ratio < alert_threshold:\n",
    "            severity = 'CRITICAL' if di_ratio < 0.7 else 'HIGH' if di_ratio < 0.8 else 'MEDIUM'\n",
    "            \n",
    "            alerts.append({\n",
    "                'attribute': attribute,\n",
    "                'severity': severity,\n",
    "                'disparate_impact': di_ratio,\n",
    "                'message': f\"{severity} bias detected in {attribute} (DI: {di_ratio:.3f})\",\n",
    "                'action_required': True if severity in ['CRITICAL', 'HIGH'] else False\n",
    "            })\n",
    "    \n",
    "    return alerts\n",
    "\n",
    "# Demonstrate monitoring functions\n",
    "print(\"‚úÖ Bias monitoring functions defined:\")\n",
    "print(\"   ‚Ä¢ continuous_bias_monitor()\")\n",
    "print(\"   ‚Ä¢ bias_alert_system()\")\n",
    "\n",
    "# Test monitoring functions\n",
    "test_protected_attrs = {'gender': 3, 'race': 4}  # Column indices\n",
    "test_results = continuous_bias_monitor(model, X_test[:100], y_test[:100], test_protected_attrs)\n",
    "test_alerts = bias_alert_system(test_results)\n",
    "\n",
    "print(f\"\\nüîç MONITORING SYSTEM TEST:\")\n",
    "print(f\"   Detected bias in {len([r for r in test_results.values() if r['bias_detected']])} attributes\")\n",
    "print(f\"   Generated {len(test_alerts)} alerts\")\n",
    "\n",
    "if test_alerts:\n",
    "    print(\"   Alert summary:\")\n",
    "    for alert in test_alerts:\n",
    "        print(f\"     {alert['severity']}: {alert['message']}\")\n",
    "\n",
    "# 8. Production Deployment Recommendations\n",
    "print(f\"\\nüöÄ PRODUCTION DEPLOYMENT RECOMMENDATIONS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"üìã BIAS DETECTION WORKFLOW:\")\n",
    "print(\"1. üìä Pre-deployment:\")\n",
    "print(\"   ‚Ä¢ Run comprehensive bias analysis on test data\")\n",
    "print(\"   ‚Ä¢ Generate bias detection report\")\n",
    "print(\"   ‚Ä¢ Review with legal/compliance team\")\n",
    "print(\"   ‚Ä¢ Set up bias monitoring thresholds\")\n",
    "\n",
    "print(\"\\n2. üîÑ Production monitoring:\")\n",
    "print(\"   ‚Ä¢ Implement continuous bias monitoring\")\n",
    "print(\"   ‚Ä¢ Set up automated alerts for bias drift\")\n",
    "print(\"   ‚Ä¢ Regular bias audits (monthly/quarterly)\")\n",
    "print(\"   ‚Ä¢ Track bias metrics over time\")\n",
    "\n",
    "print(\"\\n3. üõ†Ô∏è Bias mitigation:\")\n",
    "print(\"   ‚Ä¢ Remove protected attributes from features\")\n",
    "print(\"   ‚Ä¢ Use fairness-aware ML algorithms\")\n",
    "print(\"   ‚Ä¢ Implement post-processing fairness corrections\")\n",
    "print(\"   ‚Ä¢ Regular model retraining with bias constraints\")\n",
    "\n",
    "print(\"\\n4. üìà Governance:\")\n",
    "print(\"   ‚Ä¢ Document all bias detection procedures\")\n",
    "print(\"   ‚Ä¢ Maintain audit trails of bias metrics\")\n",
    "print(\"   ‚Ä¢ Regular stakeholder reporting\")\n",
    "print(\"   ‚Ä¢ Legal compliance verification\")\n",
    "\n",
    "print(f\"\\n‚úÖ BIAS DETECTION IMPLEMENTATION COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"You now have a comprehensive bias detection system using TrustyAI that includes:\")\n",
    "print(\"‚Ä¢ Statistical bias metrics (disparate impact)\")\n",
    "print(\"‚Ä¢ Explanation-based bias detection\")\n",
    "print(\"‚Ä¢ Continuous monitoring capabilities\")\n",
    "print(\"‚Ä¢ Automated alert systems\")\n",
    "print(\"‚Ä¢ Production-ready helper functions\")\n",
    "print(\"‚Ä¢ Comprehensive reporting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c5c6b3-cb62-439c-a538-b89b8cc6903a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
